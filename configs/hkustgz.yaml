training:
    wandb_mode: 'disabled' # [disabled, online, offline]
    test_mode: True # debug mode for training
    use_ddp: False # Use DistributedDataParallel.
    apex: False # Nvidia Apex distributed data parallel

    device: 'cuda'
    start_epoch: 0
    max_epoch: 180
    batch_size: 2 # batch size for a single gpu
    val_batch_size: 1 # batch size for a single gpu for validation

    optimizer: 
        name: 'adam' # [adam, sgd]
        lr: 0.002
        weight_decay: 0.0001
        momentum: 0.9
        amsgrad: False

    scheduler:
        name: 'poly'
        poly_exp: 1.0 # polynomial LR exponent

    model_save_dir: '/save_data/ckpt'
    snapshot: None # directory of pre-trained model
    restore_optimizer: False # Restore the saved optimizer params if snapshot is True.
    log_save_dir: '/save_data/log'

transform:
    joint_transform: True # 'Improving Semantic Segmentation via Video Propagation and Label Relaxation'
        crop_size: 720 # training crop size
        pre_size: None # resize image shorter edge to this before augmentation
        scale_min: 0.5 # dynamically scale training images down to this size
        scale_max: 2.0

        color_aug: 0.25 # level of color augmentation
        bilateral_blur: False # bilateral blur augmentation.
        gaussian_blur: True

model:
    arch: 'network.deepv3.DeepWV3Plus' # [DeepWV3Plus, DeepSRNX50V3PlusD (backbone: ResNeXt50)], deepv3: framework  

dataset:
    name: hkustgz # [hkustgz, cityscapes]
    cv_split: 3 # cross-validation split id to use
    max_skip: 0 # max_skip is used in uniform sampling/HKUSTGZUniform dataset, otherwise it is HKUSTGZ

    use_coarse_data: True # Use coarse-annotated data or not.
    coarse_boost_classes: None # use coarse annotations to boost fine data with specific classes
    dump_aug_img: False # Dump augmentated images for sanity check.
